{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56e615e",
   "metadata": {},
   "source": [
    "# Model Stealing Attack (Improved) - Assignment 2\n",
    "\n",
    "**Team Number**: 15 \n",
    "**Task**: Implement a model stealing attack against B4B-protected encoder while minimizing L2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "49fe4bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import requests\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\") # if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d0d01",
   "metadata": {},
   "source": [
    "## 1. API Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7fd0156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"50407833\" \n",
    "PORT = None\n",
    "SEED = None\n",
    "\n",
    "def launch_api():\n",
    "    # global PORT, SEED\n",
    "    response = requests.get(\n",
    "        \"http://34.122.51.94:9090/stealing_launch\",\n",
    "        headers={\"token\": TOKEN}\n",
    "    )\n",
    "    answer = response.json()\n",
    "    if 'detail' in answer:\n",
    "        raise Exception(f\"API launch failed: {answer['detail']}\")\n",
    "    SEED = str(answer['seed'])\n",
    "    # SEED = 30607040\n",
    "    PORT = str(answer['port'])\n",
    "    # PORT = 9944\n",
    "    # print(f\"API launched. Seed: {SEED}, Port: {PORT}\")\n",
    "    return SEED, PORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a145bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(images, retries=3, delay=60, timeout=120):\n",
    "    endpoint = \"/query\"\n",
    "    url = f\"http://34.122.51.94:{PORT}\" + endpoint\n",
    "    \n",
    "    # Critical: API requires exactly 1000 images\n",
    "    if len(images) != 1000:\n",
    "        print(f\"WARNING: API requires exactly 1000 images, got {len(images)}\")\n",
    "        if len(images) < 1000:\n",
    "            # Pad with duplicates of the first image to reach 1000\n",
    "            padding_needed = 1000 - len(images)\n",
    "            print(f\"Padding batch with {padding_needed} duplicate images\")\n",
    "            padding = [images[0]] * padding_needed\n",
    "            original_count = len(images)\n",
    "            images = images + padding\n",
    "        else:\n",
    "            # Truncate to exactly 1000 images\n",
    "            print(f\"Truncating batch from {len(images)} to 1000 images\")\n",
    "            images = images[:1000]\n",
    "    \n",
    "    image_data = []\n",
    "    for img in images:\n",
    "        img = transforms.ToPILImage()(img.cpu())\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        img.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr.seek(0)\n",
    "        img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
    "        image_data.append(img_base64)\n",
    "    \n",
    "    payload = json.dumps(image_data)\n",
    "    payload_size = len(payload) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"Request payload size: {payload_size:.2f} MB\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, files={\"file\": payload}, headers={\"token\": TOKEN}, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            representations = response.json()[\"representations\"]\n",
    "            \n",
    "            # If we padded the batch, return only the embeddings for original images\n",
    "            if 'original_count' in locals():\n",
    "                return representations[:original_count]\n",
    "            return representations\n",
    "            \n",
    "        elif response.status_code == 429:\n",
    "            if retries > 0:\n",
    "                print(\"Rate limited. Retrying after delay...\")\n",
    "                time.sleep(delay)\n",
    "                return query_api(images, retries - 1, delay * 2, timeout)\n",
    "            else:\n",
    "                raise Exception(\"Too many retries. Still getting rate-limited.\")\n",
    "        elif response.status_code == 400:\n",
    "            print(\"Bad request error (400). This typically means the API rejected the input format.\")\n",
    "            if retries > 0:\n",
    "                print(\"Trying again with longer timeout...\")\n",
    "                return query_api(images, retries - 1, delay, timeout * 1.5)\n",
    "            else:\n",
    "                raise Exception(\"Bad request error persisted after retries.\")\n",
    "        else:\n",
    "            raise Exception(f\"Query failed. Code: {response.status_code}\")\n",
    "            \n",
    "    except (requests.exceptions.Timeout, requests.exceptions.ReadTimeout) as e:\n",
    "        print(f\"Timeout error: {e}\")\n",
    "        # Don't split anymore since we need exactly 1000 images\n",
    "        print(\"Timeout occurred. Increasing timeout and retrying...\")\n",
    "        if retries > 0:\n",
    "            return query_api(images, retries - 1, delay, timeout * 2)\n",
    "        else:\n",
    "            raise Exception(\"Max retries exceeded with timeout errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1cb4f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New improved function: Query with repetition to reduce noise\n",
    "def query_with_repetition(images, n_repeats=3, save_prefix=\"batch\"):\n",
    "    \"\"\"Query the same images multiple times to average out noise\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(n_repeats):\n",
    "        print(f\"Repetition {i+1}/{n_repeats} for batch of {len(images)} images\")\n",
    "        embeddings = query_api(images)\n",
    "        all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Save intermediate results to avoid losing progress\n",
    "        with open(f'{save_prefix}_rep_{i+1}_embs.pkl', 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "    \n",
    "    # Average the embeddings to reduce noise\n",
    "    averaged_embeddings = []\n",
    "    for i in range(len(images)):\n",
    "        emb_list = [all_embeddings[j][i] for j in range(n_repeats)]\n",
    "        averaged_embeddings.append(np.mean(emb_list, axis=0))\n",
    "    \n",
    "    return averaged_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55a0ee",
   "metadata": {},
   "source": [
    "## 2. Coverage Tracking System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e7fd8b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageTracker:\n",
    "    def __init__(self, bucket_size=0.15, max_buckets=4096):\n",
    "        self.bucket_map = defaultdict(bool)\n",
    "        self.bucket_size = bucket_size\n",
    "        self.max_buckets = max_buckets\n",
    "        self.query_count = 0\n",
    "        self.embedding_history = []\n",
    "        \n",
    "    def _hash_embedding(self, emb):\n",
    "        emb = np.array(emb)\n",
    "        return tuple(np.floor(emb[:5] / self.bucket_size))\n",
    "        \n",
    "    def update_coverage(self, embeddings):\n",
    "        self.embedding_history.extend(embeddings)\n",
    "        for emb in embeddings:\n",
    "            self.bucket_map[self._hash_embedding(emb)] = True\n",
    "        self.query_count += len(embeddings)\n",
    "        \n",
    "    def get_coverage(self):\n",
    "        return len(self.bucket_map) / self.max_buckets\n",
    "    \n",
    "    def is_safe(self, sample_size=1000):\n",
    "        current = len(self.bucket_map)\n",
    "        projected = current + (sample_size * 0.1)\n",
    "        return projected / self.max_buckets < 0.3\n",
    "    \n",
    "    def get_embedding_stats(self):\n",
    "        \"\"\"Analyze embeddings to detect bucket boundaries\"\"\"\n",
    "        if len(self.embedding_history) < 1000:\n",
    "            print(f\"Warning: Only {len(self.embedding_history)} samples available for embedding stats\")\n",
    "            return None\n",
    "        \n",
    "        embeddings = np.array(self.embedding_history)\n",
    "        dim = embeddings.shape[1]\n",
    "        boundaries = []\n",
    "        \n",
    "        for i in range(min(dim, 10)):  # Analyze first 10 dimensions\n",
    "            values = embeddings[:, i]\n",
    "            sorted_values = np.sort(values)\n",
    "            # Detect discontinuities in the distribution\n",
    "            if len(sorted_values) > 1:\n",
    "                diffs = sorted_values[1:] - sorted_values[:-1]\n",
    "                threshold = np.percentile(diffs, 95)\n",
    "                large_gaps = np.where(diffs > threshold)[0]\n",
    "                if len(large_gaps) > 0:\n",
    "                    gap_edges = sorted_values[large_gaps + 1]\n",
    "                    boundaries.append(gap_edges)\n",
    "        \n",
    "        return boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1802470",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ac29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.ids = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.CenterCrop(32),\n",
    "            transforms.Lambda(self._ensure_rgb),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.2980, 0.2962, 0.2987), (0.2886, 0.2875, 0.2889))\n",
    "        ])\n",
    "\n",
    "    def _ensure_rgb(self, img):\n",
    "        if img.mode != 'RGB':\n",
    "            return img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def load_stealing_dataset(data_path=\"ModelStealingPub.pt\"):\n",
    "    original_data = torch.load(data_path, map_location=\"cpu\", weights_only=False)\n",
    "    dataset = TaskDataset()\n",
    "    dataset.ids = original_data.ids\n",
    "    dataset.imgs = original_data.imgs\n",
    "    dataset.labels = original_data.labels\n",
    "    return dataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle (image, embedding) pairs\"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    targets = torch.stack([item[1] for item in batch])\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5859d8",
   "metadata": {},
   "source": [
    "## 4. Improved Strategic Query Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "920c1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategic_queries(dataset, train_data, model=None, batch_size=200):\n",
    "    if len(train_data) > 0:\n",
    "        print(f\"Using {len(train_data)} training samples to guide selection\")\n",
    "    \"\"\"Strategic query selection based on uncertainty and diversity\"\"\"\n",
    "    # If we have existing data and a model, use them to guide sampling\n",
    "    if model is not None and len(train_data) > 5000:\n",
    "        # Create a PCA model from existing embeddings to identify high-variance directions\n",
    "        existing_embeddings = torch.stack([emb for _, emb in train_data]).cpu().numpy()\n",
    "        pca = PCA(n_components=min(10, existing_embeddings.shape[1]))\n",
    "        pca.fit(existing_embeddings)\n",
    "        \n",
    "        # Get dataset candidates and score them by expected information gain\n",
    "        candidate_indices = np.random.choice(len(dataset), min(10000, len(dataset)), replace=False)\n",
    "        candidates = [dataset[i] for i in candidate_indices]\n",
    "        \n",
    "        # Use current model to predict embeddings\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            candidate_tensors = torch.stack(candidates).to(device)\n",
    "            batches = torch.split(candidate_tensors, 64)\n",
    "            predictions = []\n",
    "            for batch in batches:\n",
    "                pred = model(batch).cpu().numpy()\n",
    "                predictions.extend(pred)\n",
    "            predictions = np.array(predictions)\n",
    "        \n",
    "        # Score candidates by uncertainty/projection onto principal components\n",
    "        scores = np.abs(np.dot(predictions, pca.components_.T)).sum(axis=1)\n",
    "        \n",
    "        # B4B-aware selection: avoid examples that are too similar to existing ones\n",
    "        # This helps prevent triggering the B4B defense mechanism\n",
    "        if len(train_data) > 10000:\n",
    "            existing_sample = np.array([emb.cpu().numpy() for _, emb in train_data[:5000]])\n",
    "            for i, pred in enumerate(predictions):\n",
    "                # Compute minimum distance to existing embeddings\n",
    "                min_dist = np.min(np.linalg.norm(existing_sample - pred, axis=1))\n",
    "                # Boost score for examples that are different from existing ones\n",
    "                if min_dist > 0.5:  # Threshold based on B4B bucket size\n",
    "                    scores[i] *= 1.5\n",
    "        \n",
    "        # Select most informative examples\n",
    "        selected_indices = np.argsort(scores)[-batch_size:]\n",
    "        return [candidates[i] for i in selected_indices]\n",
    "    else:\n",
    "        # If we don't have enough data yet, use farthest-first traversal for diversity\n",
    "        if len(train_data) > 0:\n",
    "            # Sample some candidates\n",
    "            candidate_indices = np.random.choice(len(dataset), min(10000, len(dataset)), replace=False)\n",
    "            candidates = [dataset[i] for i in candidate_indices]\n",
    "            \n",
    "            # Convert existing data to feature space\n",
    "            existing_images = torch.stack([img for img, _ in train_data[:min(len(train_data), 1000)]]).cpu()\n",
    "            existing_flat = existing_images.view(existing_images.size(0), -1).numpy()\n",
    "            \n",
    "            # Calculate distances\n",
    "            candidate_tensors = torch.stack(candidates)\n",
    "            candidate_flat = candidate_tensors.view(candidate_tensors.size(0), -1).numpy()\n",
    "            \n",
    "            # For each candidate, find distance to closest example in train_data\n",
    "            min_distances = []\n",
    "            for cand in candidate_flat:\n",
    "                dists = np.linalg.norm(existing_flat - cand, axis=1)\n",
    "                min_distances.append(np.min(dists))\n",
    "                \n",
    "            # Select candidates with largest minimum distance\n",
    "            selected_indices = np.argsort(min_distances)[-batch_size:]\n",
    "            return [candidates[i] for i in selected_indices]\n",
    "        else:\n",
    "            # First batch: just random\n",
    "            indices = np.random.choice(len(dataset), batch_size, replace=False)\n",
    "            return [dataset[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72e4ab",
   "metadata": {},
   "source": [
    "## 5. Improved Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedEncoderStealer(nn.Module):\n",
    "    def __init__(self, output_dim=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        # More powerful feature extractor with residual connections\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Final embedding layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.fc2 = nn.Linear(1024, output_dim)\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Third block - simplify the skip connection\n",
    "        identity = x.clone()  # Simpler skip connection\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        if identity.shape == x.shape: \n",
    "            x = x + identity\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Fourth block\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        \n",
    "        # Embedding generation\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     # First block\n",
    "    #     identity = x\n",
    "    #     x = F.relu(self.bn1(self.conv1(x)))\n",
    "    #     x = F.max_pool2d(x, 2)\n",
    "        \n",
    "    #     # Second block\n",
    "    #     x = F.relu(self.bn2(self.conv2(x)))\n",
    "    #     x = F.max_pool2d(x, 2)\n",
    "        \n",
    "    #     # Third block with skip connection if possible\n",
    "    #     identity = F.adaptive_avg_pool2d(identity, x.shape[2:])\n",
    "    #     if identity.size(1) != x.size(1):\n",
    "    #         identity = F.conv2d(identity, torch.ones(x.size(1), identity.size(1), 1, 1).to(x.device), \n",
    "    #                         padding=0, groups=identity.size(1))\n",
    "    #     identity = F.adaptive_avg_pool2d(identity, x.shape[2:])\n",
    "    #     x = F.relu(self.bn3(self.conv3(x))) + identity\n",
    "    #     x = F.max_pool2d(x, 2)\n",
    "        \n",
    "    #     # Fourth block\n",
    "    #     x = F.relu(self.bn4(self.conv4(x)))\n",
    "    #     x = F.adaptive_avg_pool2d(x, 1)\n",
    "        \n",
    "    #     # Embedding generation\n",
    "    #     x = self.flatten(x)\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     x = self.dropout(x)\n",
    "    #     x = self.fc2(x)\n",
    "        \n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "40ce2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleWrapper(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        return sum(outputs) / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c683d",
   "metadata": {},
   "source": [
    "## 6. Improved Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8a94a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_bucket_awareness(model, train_loader, bucket_edges=None, epochs=5, checkpoint_freq=1):\n",
    "    if bucket_edges is not None:\n",
    "        valid_edges = sum(1 for edges in bucket_edges if len(edges) > 0)\n",
    "        print(f\"Using {valid_edges} dimensions with detected bucket edges\")\n",
    "        \n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
    "    \n",
    "    # Use Huber loss for robustness to outliers\n",
    "    criterion = nn.SmoothL1Loss(beta=0.1)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, targets in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Standard loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Add bucket-aware regularization if bucket edges are available\n",
    "            if bucket_edges is not None:\n",
    "                bucket_loss = 0\n",
    "                for dim, edges in enumerate(bucket_edges):\n",
    "                    if len(edges) > 0 and dim < outputs.shape[1]:\n",
    "                        # Encourage outputs to be away from detected bucket edges\n",
    "                        dim_output = outputs[:, dim]\n",
    "                        for edge in edges:\n",
    "                            # Penalize being too close to bucket edge\n",
    "                            distance_to_edge = torch.abs(dim_output - edge)\n",
    "                            bucket_loss += torch.mean(torch.exp(-5 * distance_to_edge))\n",
    "                \n",
    "                if bucket_loss > 0:\n",
    "                    loss = loss + 0.01 * bucket_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent instability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint for this epoch\n",
    "        if epoch % checkpoint_freq == 0 or epoch == epochs - 1:\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(model.state_dict(), f'model_epoch_{epoch+1}_loss_{avg_loss:.4f}.pt')\n",
    "                print(f\"Saved model checkpoint at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a50b553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(train_data, n_models=3, bucket_edges=None):\n",
    "    models = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        print(f\"Training model {i+1}/{n_models}...\")\n",
    "        model = ImprovedEncoderStealer(output_dim=1024).to(device)\n",
    "        \n",
    "        # Use different subsets of data for each model for diversity\n",
    "        indices = np.random.choice(len(train_data), int(len(train_data) * 0.8), replace=False)\n",
    "        subset_data = [train_data[idx] for idx in indices]\n",
    "        \n",
    "        loader = DataLoader(subset_data, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "        train_with_bucket_awareness(model, loader, bucket_edges, epochs=3)\n",
    "        models.append(model)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c8c67124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: train_data_phase1_complete.pkl\n",
      "Warning: No matching tracker file found\n",
      "Loaded 2000 samples with 0 queries\n"
     ]
    }
   ],
   "source": [
    "# Add to imports at the top\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Recovery function\n",
    "def load_latest_checkpoint():\n",
    "    \"\"\"Load the latest saved checkpoint from disk\"\"\"\n",
    "    # Find the latest checkpoint\n",
    "    checkpoints = glob.glob('train_data_*.pkl')\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found\")\n",
    "        return None, None\n",
    "    \n",
    "    # Sort by file modification time (most recent first)\n",
    "    latest = max(checkpoints, key=os.path.getmtime)\n",
    "    print(f\"Loading checkpoint: {latest}\")\n",
    "    \n",
    "    with open(latest, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    \n",
    "    # Find corresponding tracker\n",
    "    tracker_file = latest.replace('train_data', 'tracker')\n",
    "    if os.path.exists(tracker_file):\n",
    "        with open(tracker_file, 'rb') as f:\n",
    "            tracker = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Warning: No matching tracker file found\")\n",
    "        tracker = CoverageTracker()\n",
    "        \n",
    "    print(f\"Loaded {len(train_data)} samples with {tracker.query_count} queries\")\n",
    "    return train_data, tracker\n",
    "\n",
    "# Uncomment this line to resume from checkpoint\n",
    "train_data, tracker = load_latest_checkpoint() or ([], CoverageTracker())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bacb90f",
   "metadata": {},
   "source": [
    "## 7. Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a73b3163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38215912 9944\n"
     ]
    }
   ],
   "source": [
    "# Initialize API connection\n",
    "# SEED, PORT =launch_api()\n",
    "SEED = 38215912\n",
    "PORT = 9944\n",
    "print(SEED,PORT)\n",
    "# 38215912 9944 15:27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "016efe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 13000 images\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_stealing_dataset()\n",
    "print(f\"Loaded dataset with {len(dataset)} images\")\n",
    "\n",
    "# Initialize coverage tracker\n",
    "tracker = CoverageTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb4327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Initial data collection with strategic diversity\n",
      "Loading checkpoint: train_data_phase1_complete.pkl\n",
      "Warning: No matching tracker file found\n",
      "Loaded 2000 samples with 0 queries\n",
      "Resuming from checkpoint with 2000 samples\n",
      "Processing batch 1/2\n",
      "Using 2000 training samples to guide selection\n",
      "Using repeated queries for initial batch to establish baseline...\n",
      "Repetition 1/2 for batch of 1000 images\n",
      "Request payload size: 2.55 MB\n",
      "Repetition 2/2 for batch of 1000 images\n",
      "Request payload size: 2.55 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.55 MB\n",
      "Batch 1 complete. Total samples: 3000\n",
      "Unique images: 1000\n",
      "Queries: 1000, Coverage: 1.07%\n",
      "Processing batch 2/2\n",
      "Using 3000 training samples to guide selection\n",
      "Using single queries to maximize diversity...\n",
      "Request payload size: 2.54 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.54 MB\n",
      "Batch 2 complete. Total samples: 4000\n",
      "Unique images: 2000\n",
      "Queries: 2000, Coverage: 1.10%\n",
      "Phase 1 complete: 4000 samples collected using 2000 queries\n",
      "Unique images in dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "# # Phase 1: Initial data collection with strategic diversity focus\n",
    "# print(\"Phase 1: Initial data collection with strategic diversity\")\n",
    "\n",
    "# # Use checkpoint if available, otherwise initialize\n",
    "# recovered_data = load_latest_checkpoint()\n",
    "# if recovered_data is not None and recovered_data[0]:\n",
    "#     train_data, tracker = recovered_data\n",
    "#     print(f\"Resuming from checkpoint with {len(train_data)} samples\")\n",
    "# else:\n",
    "#     train_data = []\n",
    "#     tracker = CoverageTracker()\n",
    "\n",
    "# # Track queried image IDs to avoid duplicates\n",
    "# queried_ids = set()\n",
    "\n",
    "# # More batches with smaller batch size\n",
    "# n_batches = 2  # Reduced from 8 to query fewer images with more focus on quality\n",
    "# batch_size = 1000  # Slightly larger batch size to get more coverage with fewer batches\n",
    "\n",
    "# for i in range(n_batches):\n",
    "#     try:\n",
    "#         print(f\"Processing batch {i+1}/{n_batches}\")\n",
    "#         # Use strategic query selection that prioritizes diversity\n",
    "#         batch_images = get_strategic_queries(dataset, train_data, batch_size=batch_size)\n",
    "        \n",
    "#         # Decide on repetition strategy: \n",
    "#         # - First batch: use repetition for stable initialization\n",
    "#         # - Later batches: prioritize coverage over noise reduction\n",
    "#         if i == 0:\n",
    "#             print(\"Using repeated queries for initial batch to establish baseline...\")\n",
    "#             batch_embs = query_with_repetition(batch_images, n_repeats=2, save_prefix=f\"phase1_batch_{i+1}\")\n",
    "#         else:\n",
    "#             print(\"Using single queries to maximize diversity...\")\n",
    "#             batch_embs = query_api(batch_images)\n",
    "        \n",
    "#         # Update train data and save immediately\n",
    "#         for img, emb in zip(batch_images, batch_embs):\n",
    "#             # Store image ID to track uniqueness\n",
    "#             img_id = id(img)\n",
    "#             if img_id not in queried_ids:\n",
    "#                 queried_ids.add(img_id)\n",
    "#                 train_data.append((img, torch.tensor(emb).float()))\n",
    "        \n",
    "#         # Save progress after each batch\n",
    "#         with open(f'train_data_phase1_batch_{i+1}.pkl', 'wb') as f:\n",
    "#             pickle.dump(train_data, f)\n",
    "        \n",
    "#         tracker.update_coverage(batch_embs)\n",
    "        \n",
    "#         # Save tracker state\n",
    "#         with open(f'tracker_phase1_batch_{i+1}.pkl', 'wb') as f:\n",
    "#             pickle.dump(tracker, f)\n",
    "            \n",
    "#         print(f\"Batch {i+1} complete. Total samples: {len(train_data)}\")\n",
    "#         print(f\"Unique images: {len(queried_ids)}\")\n",
    "#         print(f\"Queries: {tracker.query_count}, Coverage: {tracker.get_coverage():.2%}\")\n",
    "        \n",
    "#         # Early stopping if we have good coverage\n",
    "#         if tracker.get_coverage() > 0.08:\n",
    "#             print(\"Sufficient initial coverage achieved. Moving to Phase 2.\")\n",
    "#             break\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in batch {i+1}: {str(e)}\")\n",
    "#         # Save what we have so far\n",
    "#         if train_data:\n",
    "#             with open(f'train_data_error_phase1_{i+1}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(train_data, f)\n",
    "#             with open(f'tracker_error_phase1_{i+1}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(tracker, f)\n",
    "#         print(\"Continuing to next batch...\")\n",
    "\n",
    "# # Save final Phase 1 dataset\n",
    "# with open('train_data_phase1_complete.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "# print(f\"Phase 1 complete: {len(train_data)} samples collected using {tracker.query_count} queries\")\n",
    "# print(f\"Unique images in dataset: {len(queried_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 With B4B protection\n",
    "# Import random for jittered delays if not already imported\n",
    "import random\n",
    "\n",
    "# Function to apply strategic delays between API queries\n",
    "def apply_strategic_delay(batch_number, phase=1, high_value=False):\n",
    "    \"\"\"Apply a strategic delay between API queries to avoid triggering B4B defenses\"\"\"\n",
    "    # Base delay increases with batch number to simulate natural user behavior\n",
    "    base_delay = 5 + (batch_number * 2)\n",
    "    \n",
    "    # Add randomness to avoid predictable patterns\n",
    "    jitter = random.uniform(0.5, 1.5)\n",
    "    \n",
    "    # Phase 2 gets longer delays since model is guided and queries are more targeted\n",
    "    if phase == 2:\n",
    "        base_delay *= 1.5\n",
    "    \n",
    "    # High-value queries get additional delay to ensure quality\n",
    "    if high_value:\n",
    "        base_delay *= 1.2\n",
    "    \n",
    "    delay_time = base_delay * jitter\n",
    "    print(f\"Applying strategic delay of {delay_time:.2f}s before query (B4B avoidance)...\")\n",
    "    time.sleep(delay_time)\n",
    "\n",
    "# Phase 1: Initial data collection with strategic diversity focus\n",
    "print(\"Phase 1: Initial data collection with strategic diversity\")\n",
    "\n",
    "# Use checkpoint if available, otherwise initialize\n",
    "recovered_data = load_latest_checkpoint()\n",
    "if recovered_data is not None and recovered_data[0]:\n",
    "    train_data, tracker = recovered_data\n",
    "    print(f\"Resuming from checkpoint with {len(train_data)} samples\")\n",
    "else:\n",
    "    train_data = []\n",
    "    tracker = CoverageTracker()\n",
    "\n",
    "# Track queried image IDs to avoid duplicates\n",
    "queried_ids = set()\n",
    "\n",
    "# More batches with smaller batch size\n",
    "n_batches = 2  # Reduced from 8 to query fewer images with more focus on quality\n",
    "batch_size = 1000  # Slightly larger batch size to get more coverage with fewer batches\n",
    "\n",
    "for i in range(n_batches):\n",
    "    try:\n",
    "        print(f\"Processing batch {i+1}/{n_batches}\")\n",
    "        # Use strategic query selection that prioritizes diversity\n",
    "        batch_images = get_strategic_queries(dataset, train_data, batch_size=batch_size)\n",
    "        \n",
    "        # Apply strategic delay before API query to avoid B4B detection\n",
    "        apply_strategic_delay(i, phase=1)\n",
    "        \n",
    "        # Decide on repetition strategy: \n",
    "        # - First batch: use repetition for stable initialization\n",
    "        # - Later batches: prioritize coverage over noise reduction\n",
    "        if i == 0:\n",
    "            print(\"Using repeated queries for initial batch to establish baseline...\")\n",
    "            batch_embs = query_with_repetition(batch_images, n_repeats=2, save_prefix=f\"phase1_batch_{i+1}\")\n",
    "        else:\n",
    "            print(\"Using single queries to maximize diversity...\")\n",
    "            batch_embs = query_api(batch_images)\n",
    "        \n",
    "        # Update train data and save immediately\n",
    "        for img, emb in zip(batch_images, batch_embs):\n",
    "            # Store image ID to track uniqueness\n",
    "            img_id = id(img)\n",
    "            if img_id not in queried_ids:\n",
    "                queried_ids.add(img_id)\n",
    "                train_data.append((img, torch.tensor(emb).float()))\n",
    "        \n",
    "        # Save progress after each batch\n",
    "        with open(f'train_data_phase1_batch_{i+1}.pkl', 'wb') as f:\n",
    "            pickle.dump(train_data, f)\n",
    "        \n",
    "        tracker.update_coverage(batch_embs)\n",
    "        \n",
    "        # Save tracker state\n",
    "        with open(f'tracker_phase1_batch_{i+1}.pkl', 'wb') as f:\n",
    "            pickle.dump(tracker, f)\n",
    "            \n",
    "        print(f\"Batch {i+1} complete. Total samples: {len(train_data)}\")\n",
    "        print(f\"Unique images: {len(queried_ids)}\")\n",
    "        print(f\"Queries: {tracker.query_count}, Coverage: {tracker.get_coverage():.2%}\")\n",
    "        \n",
    "        # Early stopping if we have good coverage\n",
    "        if tracker.get_coverage() > 0.08:\n",
    "            print(\"Sufficient initial coverage achieved. Moving to Phase 2.\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i+1}: {str(e)}\")\n",
    "        # Save what we have so far\n",
    "        if train_data:\n",
    "            with open(f'train_data_error_phase1_{i+1}.pkl', 'wb') as f:\n",
    "                pickle.dump(train_data, f)\n",
    "            with open(f'tracker_error_phase1_{i+1}.pkl', 'wb') as f:\n",
    "                pickle.dump(tracker, f)\n",
    "        print(\"Continuing to next batch...\")\n",
    "\n",
    "# Save final Phase 1 dataset\n",
    "with open('train_data_phase1_complete.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "print(f\"Phase 1 complete: {len(train_data)} samples collected using {tracker.query_count} queries\")\n",
    "print(f\"Unique images in dataset: {len(queried_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1a5bc875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 100 samples, found 100 unique embedding patterns\n"
     ]
    }
   ],
   "source": [
    "# Add this to check embedding diversity across your dataset\n",
    "def check_embedding_diversity(train_data, sample_size=100):\n",
    "    if len(train_data) < sample_size:\n",
    "        sample_size = len(train_data)\n",
    "    \n",
    "    # Sample random indices\n",
    "    indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
    "    \n",
    "    # Collect embeddings\n",
    "    embeddings = [train_data[i][1].cpu().numpy() for i in indices]\n",
    "    \n",
    "    # Check uniqueness\n",
    "    unique_embeddings = set()\n",
    "    for emb in embeddings:\n",
    "        # Use hash of rounded values to check approximate uniqueness\n",
    "        emb_hash = hash(tuple(np.round(emb[:10], 3)))\n",
    "        unique_embeddings.add(emb_hash)\n",
    "    \n",
    "    print(f\"Checked {sample_size} samples, found {len(unique_embeddings)} unique embedding patterns\")\n",
    "    return len(unique_embeddings) / sample_size\n",
    "\n",
    "# Call this before training\n",
    "diversity_ratio = check_embedding_diversity(train_data)\n",
    "if diversity_ratio < 0.5:\n",
    "    print(\"WARNING: Low embedding diversity detected. Model training may be ineffective.\")\n",
    "    print(\"Consider gathering more diverse examples before proceeding.\")\n",
    "    \n",
    "# Modify training parameters based on diversity\n",
    "initial_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=64,  # Smaller batch size for stability\n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Use simpler loss function if diversity is low\n",
    "criterion = nn.MSELoss() if diversity_ratio < 0.3 else nn.SmoothL1Loss(beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e2fa6e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 4000 samples (63 batches)\n",
      "Epoch 1, Batch 10/63, Loss: 0.3057\n",
      "Epoch 1, Batch 20/63, Loss: 0.2004\n",
      "Epoch 1, Batch 30/63, Loss: 0.1546\n",
      "Epoch 1, Batch 40/63, Loss: 0.1282\n",
      "Epoch 1, Batch 50/63, Loss: 0.1124\n",
      "Epoch 1, Batch 60/63, Loss: 0.1009\n",
      "Epoch 1 complete, Average loss: 0.0980\n",
      "Epoch 2, Batch 10/63, Loss: 0.0363\n",
      "Epoch 2, Batch 20/63, Loss: 0.0350\n",
      "Epoch 2, Batch 30/63, Loss: 0.0363\n",
      "Epoch 2, Batch 40/63, Loss: 0.0357\n",
      "Epoch 2, Batch 50/63, Loss: 0.0359\n",
      "Epoch 2, Batch 60/63, Loss: 0.0358\n",
      "Epoch 2 complete, Average loss: 0.0355\n",
      "Epoch 3, Batch 10/63, Loss: 0.0320\n",
      "Epoch 3, Batch 20/63, Loss: 0.0329\n",
      "Epoch 3, Batch 30/63, Loss: 0.0318\n",
      "Epoch 3, Batch 40/63, Loss: 0.0314\n",
      "Epoch 3, Batch 50/63, Loss: 0.0307\n",
      "Epoch 3, Batch 60/63, Loss: 0.0304\n",
      "Epoch 3 complete, Average loss: 0.0305\n"
     ]
    }
   ],
   "source": [
    "# Replace with this more robust implementation\n",
    "device = torch.device(\"cpu\")  # Consistently use CPU for stability\n",
    "\n",
    "model = ImprovedEncoderStealer(output_dim=1024).to(device)\n",
    "criterion = nn.SmoothL1Loss(beta=0.1)  # Use this since diversity is high\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Add proper progress tracking\n",
    "total_batches = len(initial_loader)\n",
    "print(f\"Starting training with {len(train_data)} samples ({total_batches} batches)\")\n",
    "\n",
    "# Basic training loop with more information\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for images, targets in initial_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # Add gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Better progress reporting\n",
    "        running_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        if batch_count % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_count}/{total_batches}, Loss: {running_loss/batch_count:.4f}\")\n",
    "    \n",
    "    # Save checkpoint after each epoch\n",
    "    avg_loss = running_loss / batch_count\n",
    "    print(f\"Epoch {epoch+1} complete, Average loss: {avg_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "124a4a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc6853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 2: Strategic query expansion with dataset awareness\n",
      "Target unique images: 9000, Current unique images: 2000\n",
      "Using 4000 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 1000 images\n",
      "Request payload size: 2.54 MB\n",
      "Repetition 2/2 for batch of 1000 images\n",
      "Request payload size: 2.54 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.54 MB\n",
      "Saved checkpoint with 5000 samples\n",
      "Queries: 3000, Coverage: 1.10%\n",
      "Unique images: 3000/9000\n",
      "Using 5000 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 1000 images\n",
      "Request payload size: 2.55 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.55 MB\n",
      "Repetition 2/2 for batch of 1000 images\n",
      "Request payload size: 2.55 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.55 MB\n",
      "Saved checkpoint with 6000 samples\n",
      "\n",
      "Retraining with 6000 samples...\n",
      "Using 10 dimensions with detected bucket edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:13<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8005\n",
      "Saved model checkpoint at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:13<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.2294\n",
      "Saved model checkpoint at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:13<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1361\n",
      "Saved model checkpoint at epoch 3\n",
      "Queries: 4000, Coverage: 1.10%\n",
      "Unique images: 4000/9000\n",
      "Using 6000 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 1000 images\n",
      "Request payload size: 2.70 MB\n",
      "Repetition 2/2 for batch of 1000 images\n",
      "Request payload size: 2.70 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.70 MB\n",
      "Error during querying: ('Connection aborted.', TimeoutError('timed out'))\n",
      "Saving current progress and continuing...\n",
      "Using 6000 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 1000 images\n",
      "Request payload size: 2.70 MB\n",
      "Error during querying: ('Connection aborted.', TimeoutError('timed out'))\n",
      "Saving current progress and continuing...\n",
      "Using 6000 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 1000 images\n",
      "Request payload size: 2.69 MB\n",
      "Repetition 2/2 for batch of 1000 images\n",
      "Request payload size: 2.69 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.69 MB\n",
      "Saved checkpoint with 7000 samples\n",
      "Queries: 5000, Coverage: 1.27%\n",
      "Unique images: 7000/9000\n",
      "Using 7000 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 841 images\n",
      "WARNING: API requires exactly 1000 images, got 841\n",
      "Padding batch with 159 duplicate images\n",
      "Request payload size: 2.70 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.70 MB\n",
      "Repetition 2/2 for batch of 841 images\n",
      "WARNING: API requires exactly 1000 images, got 841\n",
      "Padding batch with 159 duplicate images\n",
      "Request payload size: 2.70 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.70 MB\n",
      "Queries: 5841, Coverage: 1.29%\n",
      "Unique images: 7841/9000\n",
      "Using 7841 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 880 images\n",
      "WARNING: API requires exactly 1000 images, got 880\n",
      "Padding batch with 120 duplicate images\n",
      "Request payload size: 2.66 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.66 MB\n",
      "Repetition 2/2 for batch of 880 images\n",
      "WARNING: API requires exactly 1000 images, got 880\n",
      "Padding batch with 120 duplicate images\n",
      "Request payload size: 2.66 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.66 MB\n",
      "Queries: 6721, Coverage: 1.29%\n",
      "Unique images: 8721/9000\n",
      "Using 8721 training samples to guide selection\n",
      "Using repeated queries for high-value samples\n",
      "Repetition 1/2 for batch of 895 images\n",
      "WARNING: API requires exactly 1000 images, got 895\n",
      "Padding batch with 105 duplicate images\n",
      "Request payload size: 2.75 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.75 MB\n",
      "Repetition 2/2 for batch of 895 images\n",
      "WARNING: API requires exactly 1000 images, got 895\n",
      "Padding batch with 105 duplicate images\n",
      "Request payload size: 2.75 MB\n",
      "Rate limited. Retrying after delay...\n",
      "Request payload size: 2.75 MB\n",
      "Queries: 7616, Coverage: 1.29%\n",
      "Unique images: 9616/9000\n",
      "Phase 2 complete: 9616 samples collected using 7616 queries\n",
      "Final unique images queried: 9616\n"
     ]
    }
   ],
   "source": [
    "# # Phase 2: Strategic queries with model guidance and dataset limits\n",
    "# print(\"\\nPhase 2: Strategic query expansion with dataset awareness\")\n",
    "\n",
    "# # Configure Phase 2 parameters\n",
    "# max_queries = min(95000, tracker.query_count + 50000)  # Keep some quota for final phases\n",
    "# batch_size = 1000  # Smaller batch size to reduce timeout risk\n",
    "# save_frequency = 500  # Save more frequently\n",
    "# max_unique_images = min(9000, len(dataset) * 0.7)  # Target 70% of dataset\n",
    "\n",
    "# # Keep track of unique images we've queried\n",
    "# if 'queried_ids' not in locals():\n",
    "#     queried_ids = set([id(img) for img, _ in train_data])\n",
    "#     print(f\"Identified {len(queried_ids)} unique images in current dataset\")\n",
    "\n",
    "# # Detect initial bucket boundaries for improved training\n",
    "# if tracker.query_count > 2000:  # Only if we have enough data\n",
    "#     bucket_edges = tracker.get_embedding_stats()\n",
    "#     print(\"Bucket edges detected for improved query selection\")\n",
    "\n",
    "# # Define a tracker for high-quality samples that might need repetition\n",
    "# high_value_queries = 0\n",
    "# max_high_value_queries = 10000  # Limit for high-value repeated queries\n",
    "\n",
    "# print(f\"Target unique images: {max_unique_images}, Current unique images: {len(queried_ids)}\")\n",
    "\n",
    "# while (tracker.query_count < max_queries and \n",
    "#        tracker.get_coverage() < 0.25 and \n",
    "#        len(queried_ids) < max_unique_images):\n",
    "#     try:\n",
    "#         # Use the model to guide query selection\n",
    "#         batch_images = get_strategic_queries(dataset, train_data, model, batch_size=batch_size)\n",
    "        \n",
    "#         # Filter out any images we've already queried\n",
    "#         new_batch_images = []\n",
    "#         for img in batch_images:\n",
    "#             img_id = id(img)\n",
    "#             if img_id not in queried_ids:\n",
    "#                 new_batch_images.append(img)\n",
    "#                 queried_ids.add(img_id)\n",
    "        \n",
    "#         if len(new_batch_images) == 0:\n",
    "#             print(\"All candidates already queried. Increasing sampling pool...\")\n",
    "#             candidate_indices = np.random.choice(len(dataset), min(5000, len(dataset)), replace=False)\n",
    "#             candidates = [dataset[i] for i in candidate_indices]\n",
    "#             new_batch_images = [img for img in candidates if id(img) not in queried_ids][:batch_size]\n",
    "            \n",
    "#             if len(new_batch_images) == 0:\n",
    "#                 print(\"Dataset exhausted. Moving to final training phase.\")\n",
    "#                 break\n",
    "        \n",
    "#         # Decide on query strategy based on PCA analysis of current embeddings\n",
    "#         use_repetition = False\n",
    "#         if high_value_queries < max_high_value_queries:\n",
    "#             # Check if this batch might contain valuable samples based on model uncertainty\n",
    "#             if model is not None and len(train_data) > 2000:\n",
    "#                 with torch.no_grad():\n",
    "#                     model.eval()\n",
    "#                     batch_tensor = torch.stack(new_batch_images).to(device)\n",
    "#                     predictions = model(batch_tensor).cpu().numpy()\n",
    "                    \n",
    "#                     # Get embeddings from existing data for comparison\n",
    "#                     existing_embs = np.array([emb.cpu().numpy() for _, emb in train_data[:2000]])\n",
    "                    \n",
    "#                     # Check if predictions are in underrepresented regions\n",
    "#                     distances = []\n",
    "#                     for pred in predictions:\n",
    "#                         min_dist = np.min(np.linalg.norm(existing_embs - pred, axis=1))\n",
    "#                         distances.append(min_dist)\n",
    "                    \n",
    "#                     # If average distance is high, these samples could be valuable\n",
    "#                     if np.mean(distances) > 0.8:  # High threshold for repetition\n",
    "#                         use_repetition = True\n",
    "#                         high_value_queries += len(new_batch_images)\n",
    "        \n",
    "#         # Query with or without repetition based on analysis\n",
    "#         if use_repetition:\n",
    "#             print(f\"Using repeated queries for high-value samples\")\n",
    "#             batch_embs = query_with_repetition(\n",
    "#                 new_batch_images, \n",
    "#                 n_repeats=2, \n",
    "#                 save_prefix=f\"phase2_batch_hv_{len(train_data)}\"\n",
    "#             )\n",
    "#         else:\n",
    "#             print(f\"Using single queries to maximize coverage\")\n",
    "#             batch_embs = query_api(new_batch_images)\n",
    "        \n",
    "#         # Update training data\n",
    "#         for img, emb in zip(new_batch_images, batch_embs):\n",
    "#             train_data.append((img, torch.tensor(emb).float()))\n",
    "        \n",
    "#         # Save checkpoint periodically\n",
    "#         if len(train_data) % save_frequency == 0:\n",
    "#             with open(f'train_data_phase2_{len(train_data)}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(train_data, f)\n",
    "#             with open(f'tracker_phase2_{tracker.query_count}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(tracker, f)\n",
    "#             print(f\"Saved checkpoint with {len(train_data)} samples\")\n",
    "        \n",
    "#         tracker.update_coverage(batch_embs)\n",
    "        \n",
    "#         # Refresh bucket boundaries periodically\n",
    "#         if len(train_data) % 5000 == 0:\n",
    "#             bucket_edges = tracker.get_embedding_stats()\n",
    "        \n",
    "#         # Retrain more frequently with smaller increments\n",
    "#         if len(train_data) % 3000 == 0:  # More frequent retraining\n",
    "#             print(f\"\\nRetraining with {len(train_data)} samples...\")\n",
    "#             loader = DataLoader(train_data, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "#             model = ImprovedEncoderStealer(output_dim=1024).to(device)\n",
    "#             train_with_bucket_awareness(model, loader, bucket_edges, epochs=3)\n",
    "            \n",
    "#             # Save this intermediate model\n",
    "#             torch.save(model.state_dict(), f'model_phase2_{len(train_data)}.pt')\n",
    "        \n",
    "#         print(f\"Queries: {tracker.query_count}, Coverage: {tracker.get_coverage():.2%}\")\n",
    "#         print(f\"Unique images: {len(queried_ids)}/{max_unique_images}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during querying: {e}\")\n",
    "#         print(\"Saving current progress and continuing...\")\n",
    "#         with open(f'train_data_error_phase2_{len(train_data)}.pkl', 'wb') as f:\n",
    "#             pickle.dump(train_data, f)\n",
    "#         with open(f'tracker_error_phase2_{tracker.query_count}.pkl', 'wb') as f:\n",
    "#             pickle.dump(tracker, f)\n",
    "#         time.sleep(30)  # Wait before retrying\n",
    "\n",
    "# # Save final Phase 2 dataset\n",
    "# with open('train_data_phase2_complete.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "# print(f\"Phase 2 complete: {len(train_data)} samples collected using {tracker.query_count} queries\")\n",
    "# print(f\"Final unique images queried: {len(queried_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 with B4B protection: Strategic queries with model guidance and dataset limits\n",
    "print(\"\\nPhase 2: Strategic query expansion with dataset awareness\")\n",
    "\n",
    "# Configure Phase 2 parameters\n",
    "max_queries = min(95000, tracker.query_count + 50000)  # Keep some quota for final phases\n",
    "batch_size = 1000  # Smaller batch size to reduce timeout risk\n",
    "save_frequency = 500  # Save more frequently\n",
    "max_unique_images = min(9000, len(dataset) * 0.7)  # Target 70% of dataset\n",
    "\n",
    "# Keep track of unique images we've queried\n",
    "if 'queried_ids' not in locals():\n",
    "    queried_ids = set([id(img) for img, _ in train_data])\n",
    "    print(f\"Identified {len(queried_ids)} unique images in current dataset\")\n",
    "\n",
    "# Detect initial bucket boundaries for improved training\n",
    "if tracker.query_count > 2000:  # Only if we have enough data\n",
    "    print(\"Detecting bucket boundaries - adding longer delay to avoid B4B...\")\n",
    "    time.sleep(30)  # Longer cool-down period before sensitive operations\n",
    "    bucket_edges = tracker.get_embedding_stats()\n",
    "    print(\"Bucket edges detected for improved query selection\")\n",
    "\n",
    "# Define a tracker for high-quality samples that might need repetition\n",
    "high_value_queries = 0\n",
    "max_high_value_queries = 10000  # Limit for high-value repeated queries\n",
    "batch_counter = 0  # To track batch number for strategic delays\n",
    "\n",
    "print(f\"Target unique images: {max_unique_images}, Current unique images: {len(queried_ids)}\")\n",
    "\n",
    "while (tracker.query_count < max_queries and \n",
    "       tracker.get_coverage() < 0.25 and \n",
    "       len(queried_ids) < max_unique_images):\n",
    "    try:\n",
    "        # Use the model to guide query selection\n",
    "        batch_images = get_strategic_queries(dataset, train_data, model, batch_size=batch_size)\n",
    "        \n",
    "        # Filter out any images we've already queried\n",
    "        new_batch_images = []\n",
    "        for img in batch_images:\n",
    "            img_id = id(img)\n",
    "            if img_id not in queried_ids:\n",
    "                new_batch_images.append(img)\n",
    "                queried_ids.add(img_id)\n",
    "        \n",
    "        if len(new_batch_images) == 0:\n",
    "            print(\"All candidates already queried. Increasing sampling pool...\")\n",
    "            candidate_indices = np.random.choice(len(dataset), min(5000, len(dataset)), replace=False)\n",
    "            candidates = [dataset[i] for i in candidate_indices]\n",
    "            new_batch_images = [img for img in candidates if id(img) not in queried_ids][:batch_size]\n",
    "            \n",
    "            if len(new_batch_images) == 0:\n",
    "                print(\"Dataset exhausted. Moving to final training phase.\")\n",
    "                break\n",
    "        \n",
    "        # Decide on query strategy based on PCA analysis of current embeddings\n",
    "        use_repetition = False\n",
    "        if high_value_queries < max_high_value_queries:\n",
    "            # Check if this batch might contain valuable samples based on model uncertainty\n",
    "            if model is not None and len(train_data) > 2000:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    batch_tensor = torch.stack(new_batch_images).to(device)\n",
    "                    predictions = model(batch_tensor).cpu().numpy()\n",
    "                    \n",
    "                    # Get embeddings from existing data for comparison\n",
    "                    existing_embs = np.array([emb.cpu().numpy() for _, emb in train_data[:2000]])\n",
    "                    \n",
    "                    # Check if predictions are in underrepresented regions\n",
    "                    distances = []\n",
    "                    for pred in predictions:\n",
    "                        min_dist = np.min(np.linalg.norm(existing_embs - pred, axis=1))\n",
    "                        distances.append(min_dist)\n",
    "                    \n",
    "                    # If average distance is high, these samples could be valuable\n",
    "                    if np.mean(distances) > 0.8:  # High threshold for repetition\n",
    "                        use_repetition = True\n",
    "                        high_value_queries += len(new_batch_images)\n",
    "        \n",
    "        # Apply strategic delay before API query, longer for high-value queries\n",
    "        apply_strategic_delay(batch_counter, phase=2, high_value=use_repetition)\n",
    "        batch_counter += 1\n",
    "        \n",
    "        # Query with or without repetition based on analysis\n",
    "        if use_repetition:\n",
    "            print(f\"Using repeated queries for high-value samples\")\n",
    "            batch_embs = query_with_repetition(\n",
    "                new_batch_images, \n",
    "                n_repeats=2, \n",
    "                save_prefix=f\"phase2_batch_hv_{len(train_data)}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Using single queries to maximize coverage\")\n",
    "            batch_embs = query_api(new_batch_images)\n",
    "        \n",
    "        # Update training data\n",
    "        for img, emb in zip(new_batch_images, batch_embs):\n",
    "            train_data.append((img, torch.tensor(emb).float()))\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if len(train_data) % save_frequency == 0:\n",
    "            with open(f'train_data_phase2_{len(train_data)}.pkl', 'wb') as f:\n",
    "                pickle.dump(train_data, f)\n",
    "            with open(f'tracker_phase2_{tracker.query_count}.pkl', 'wb') as f:\n",
    "                pickle.dump(tracker, f)\n",
    "            print(f\"Saved checkpoint with {len(train_data)} samples\")\n",
    "        \n",
    "        tracker.update_coverage(batch_embs)\n",
    "        \n",
    "        # Refresh bucket boundaries periodically with a longer delay\n",
    "        if len(train_data) % 5000 == 0:\n",
    "            print(\"Refreshing bucket boundaries - adding longer delay to avoid B4B...\")\n",
    "            time.sleep(30)  # Longer cool-down period before sensitive operations\n",
    "            bucket_edges = tracker.get_embedding_stats()\n",
    "        \n",
    "        # Retrain more frequently with smaller increments\n",
    "        if len(train_data) % 3000 == 0:  # More frequent retraining\n",
    "            print(f\"\\nRetraining with {len(train_data)} samples...\")\n",
    "            loader = DataLoader(train_data, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "            model = ImprovedEncoderStealer(output_dim=1024).to(device)\n",
    "            train_with_bucket_awareness(model, loader, bucket_edges, epochs=3)\n",
    "            \n",
    "            # Save this intermediate model\n",
    "            torch.save(model.state_dict(), f'model_phase2_{len(train_data)}.pt')\n",
    "        \n",
    "        print(f\"Queries: {tracker.query_count}, Coverage: {tracker.get_coverage():.2%}\")\n",
    "        print(f\"Unique images: {len(queried_ids)}/{max_unique_images}\")\n",
    "        \n",
    "        # Add a variable cool-down period between batches to avoid patterns\n",
    "        cool_down = random.uniform(5, 15)\n",
    "        print(f\"Cooling down for {cool_down:.2f}s to avoid B4B detection...\")\n",
    "        time.sleep(cool_down)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during querying: {e}\")\n",
    "        print(\"Saving current progress and continuing...\")\n",
    "        with open(f'train_data_error_phase2_{len(train_data)}.pkl', 'wb') as f:\n",
    "            pickle.dump(train_data, f)\n",
    "        with open(f'tracker_error_phase2_{tracker.query_count}.pkl', 'wb') as f:\n",
    "            pickle.dump(tracker, f)\n",
    "        time.sleep(30)  # Wait before retrying\n",
    "\n",
    "# Save final Phase 2 dataset\n",
    "with open('train_data_phase2_complete.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "print(f\"Phase 2 complete: {len(train_data)} samples collected using {tracker.query_count} queries\")\n",
    "print(f\"Final unique images queried: {len(queried_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a08a0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ensemble models...\n",
      "Using 10 dimensions with detected bucket edges\n",
      "Saved final dataset with 9616 samples\n",
      "Training model 1/3...\n",
      "Using 10 dimensions with detected bucket edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:20<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9342\n",
      "Saved model checkpoint at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:19<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.2108\n",
      "Saved model checkpoint at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:20<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1448\n",
      "Saved model checkpoint at epoch 3\n",
      "Training model 2/3...\n",
      "Using 10 dimensions with detected bucket edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:18<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7840\n",
      "Saved model checkpoint at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:20<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1832\n",
      "Saved model checkpoint at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:21<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1214\n",
      "Saved model checkpoint at epoch 3\n",
      "Training model 3/3...\n",
      "Using 10 dimensions with detected bucket edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:19<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7764\n",
      "Saved model checkpoint at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:21<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1796\n",
      "Saved model checkpoint at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:19<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1203\n",
      "Saved model checkpoint at epoch 3\n",
      "Saved ensemble model 1\n",
      "Saved ensemble model 2\n",
      "Saved ensemble model 3\n",
      "Saved final ensemble model\n"
     ]
    }
   ],
   "source": [
    "# Final ensemble training\n",
    "print(\"\\nTraining ensemble models...\")\n",
    "\n",
    "# Get bucket boundaries for final training\n",
    "bucket_edges = tracker.get_embedding_stats()\n",
    "print(f\"Using {len([e for e in bucket_edges if len(e) > 0])} dimensions with detected bucket edges\")\n",
    "\n",
    "# Save full dataset before training\n",
    "with open('train_data_final.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "print(f\"Saved final dataset with {len(train_data)} samples\")\n",
    "\n",
    "# Train ensemble models (no changes to the function itself)\n",
    "models = train_ensemble(train_data, n_models=3, bucket_edges=bucket_edges)\n",
    "\n",
    "# Save individual models\n",
    "for i, model in enumerate(models):\n",
    "    torch.save(model.state_dict(), f'ensemble_model_{i+1}.pt')\n",
    "    print(f\"Saved ensemble model {i+1}\")\n",
    "\n",
    "# Create and save wrapper model\n",
    "final_model = EnsembleWrapper(models).to(device)\n",
    "torch.save(final_model.state_dict(), 'final_ensemble_model.pt')\n",
    "print(\"Saved final ensemble model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825fdce",
   "metadata": {},
   "source": [
    "## 8. Model Validation and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0af7ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_onnx(model_path):\n",
    "    try:\n",
    "        session = ort.InferenceSession(model_path)\n",
    "        # Check input name matches 'x'\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        assert input_name == \"x\", f\"Input name should be 'x', got {input_name}\"\n",
    "        \n",
    "        test_input = np.random.randn(1, 3, 32, 32).astype(np.float32)\n",
    "        output = session.run(None, {\"x\": test_input})[0]  # Note using \"x\" here\n",
    "        \n",
    "        assert output.shape == (1, 1024)\n",
    "        print(\"ONNX validation passed! Model meets submission requirements\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"ONNX validation failed: {str(e)}\")\n",
    "        \n",
    "def submit_model(model_path):\n",
    "    url = \"http://34.122.51.94:9090/stealing\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        files = {\"file\": f}\n",
    "        headers = {\"token\": TOKEN, \"seed\": str(SEED)}\n",
    "        response = requests.post(url, files=files, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Submission successful!\")\n",
    "        print(response.json())\n",
    "    else:\n",
    "        print(f\"Submission failed: {response.status_code}\")\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "14f066bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing for submission...\n",
      "Model test - Input shape: torch.Size([1, 3, 32, 32]), Output shape: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Verify model output shape\n",
    "print(\"\\nPreparing for submission...\")\n",
    "test_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "with torch.no_grad():\n",
    "    test_output = final_model(test_input)\n",
    "print(f\"Model test - Input shape: {test_input.shape}, Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "22e82ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/_mtd2_kx7s174908b5csl7yc0000gn/T/ipykernel_23142/2169857439.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if identity.shape == x.shape:  # Only add if shapes match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%x : Float(*, 3, 32, 32, strides=[3072, 1024, 32, 1], requires_grad=0, device=cpu),\n",
      "      %models.0.fc1.weight : Float(1024, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %models.0.fc1.bias : Float(1024, strides=[1], requires_grad=1, device=cpu),\n",
      "      %models.0.fc2.weight : Float(1024, 1024, strides=[1024, 1], requires_grad=1, device=cpu),\n",
      "      %models.0.fc2.bias : Float(1024, strides=[1], requires_grad=1, device=cpu),\n",
      "      %models.1.fc1.weight : Float(1024, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %models.1.fc1.bias : Float(1024, strides=[1], requires_grad=1, device=cpu),\n",
      "      %models.1.fc2.weight : Float(1024, 1024, strides=[1024, 1], requires_grad=1, device=cpu),\n",
      "      %models.1.fc2.bias : Float(1024, strides=[1], requires_grad=1, device=cpu),\n",
      "      %models.2.fc1.weight : Float(1024, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %models.2.fc1.bias : Float(1024, strides=[1], requires_grad=1, device=cpu),\n",
      "      %models.2.fc2.weight : Float(1024, 1024, strides=[1024, 1], requires_grad=1, device=cpu),\n",
      "      %models.2.fc2.bias : Float(1024, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_162 : Float(64, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_163 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_165 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_166 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_168 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_169 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_171 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_172 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_174 : Float(64, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_175 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_177 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_178 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_180 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_181 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_183 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_184 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_186 : Float(64, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_187 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_189 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_190 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_192 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_193 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_195 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_196 : Float(512, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/models.0/conv1/Conv_output_0 : Float(*, 64, 32, 32, strides=[65536, 1024, 32, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.0/conv1/Conv\"](%x, %onnx::Conv_162, %onnx::Conv_163), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0/torch.nn.modules.conv.Conv2d::conv1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.0/Relu_output_0 : Float(*, 64, 32, 32, strides=[65536, 1024, 32, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.0/Relu\"](%/models.0/conv1/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.0/MaxPool_output_0 : Float(*, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.0/MaxPool\"](%/models.0/Relu_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.0/conv2/Conv_output_0 : Float(*, 128, 16, 16, strides=[32768, 256, 16, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.0/conv2/Conv\"](%/models.0/MaxPool_output_0, %onnx::Conv_165, %onnx::Conv_166), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0/torch.nn.modules.conv.Conv2d::conv2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.0/Relu_1_output_0 : Float(*, 128, 16, 16, strides=[32768, 256, 16, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.0/Relu_1\"](%/models.0/conv2/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.0/MaxPool_1_output_0 : Float(*, 128, 8, 8, strides=[8192, 64, 8, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.0/MaxPool_1\"](%/models.0/Relu_1_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.0/conv3/Conv_output_0 : Float(*, 256, 8, 8, strides=[16384, 64, 8, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.0/conv3/Conv\"](%/models.0/MaxPool_1_output_0, %onnx::Conv_168, %onnx::Conv_169), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0/torch.nn.modules.conv.Conv2d::conv3 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.0/Relu_2_output_0 : Float(*, 256, 8, 8, strides=[16384, 64, 8, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.0/Relu_2\"](%/models.0/conv3/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.0/MaxPool_2_output_0 : Float(*, 256, 4, 4, strides=[4096, 16, 4, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.0/MaxPool_2\"](%/models.0/Relu_2_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.0/conv4/Conv_output_0 : Float(*, 512, 4, 4, strides=[8192, 16, 4, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.0/conv4/Conv\"](%/models.0/MaxPool_2_output_0, %onnx::Conv_171, %onnx::Conv_172), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0/torch.nn.modules.conv.Conv2d::conv4 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.0/Relu_3_output_0 : Float(*, 512, 4, 4, strides=[8192, 16, 4, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.0/Relu_3\"](%/models.0/conv4/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.0/GlobalAveragePool_output_0 : Float(*, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool[onnx_name=\"/models.0/GlobalAveragePool\"](%/models.0/Relu_3_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1382:0\n",
      "  %/models.0/flatten/Flatten_output_0 : Float(*, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/models.0/flatten/Flatten\"](%/models.0/GlobalAveragePool_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0/torch.nn.modules.flatten.Flatten::flatten # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/flatten.py:53:0\n",
      "  %/models.0/fc1/Gemm_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/models.0/fc1/Gemm\"](%/models.0/flatten/Flatten_output_0, %models.0.fc1.weight, %models.0.fc1.bias), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0/torch.nn.modules.linear.Linear::fc1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/models.0/Relu_4_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.0/Relu_4\"](%/models.0/fc1/Gemm_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.0/fc2/Gemm_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/models.0/fc2/Gemm\"](%/models.0/Relu_4_output_0, %models.0.fc2.weight, %models.0.fc2.bias), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.0/torch.nn.modules.linear.Linear::fc2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/models.1/conv1/Conv_output_0 : Float(*, 64, 32, 32, strides=[65536, 1024, 32, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.1/conv1/Conv\"](%x, %onnx::Conv_174, %onnx::Conv_175), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1/torch.nn.modules.conv.Conv2d::conv1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.1/Relu_output_0 : Float(*, 64, 32, 32, strides=[65536, 1024, 32, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.1/Relu\"](%/models.1/conv1/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.1/MaxPool_output_0 : Float(*, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.1/MaxPool\"](%/models.1/Relu_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.1/conv2/Conv_output_0 : Float(*, 128, 16, 16, strides=[32768, 256, 16, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.1/conv2/Conv\"](%/models.1/MaxPool_output_0, %onnx::Conv_177, %onnx::Conv_178), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1/torch.nn.modules.conv.Conv2d::conv2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.1/Relu_1_output_0 : Float(*, 128, 16, 16, strides=[32768, 256, 16, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.1/Relu_1\"](%/models.1/conv2/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.1/MaxPool_1_output_0 : Float(*, 128, 8, 8, strides=[8192, 64, 8, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.1/MaxPool_1\"](%/models.1/Relu_1_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.1/conv3/Conv_output_0 : Float(*, 256, 8, 8, strides=[16384, 64, 8, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.1/conv3/Conv\"](%/models.1/MaxPool_1_output_0, %onnx::Conv_180, %onnx::Conv_181), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1/torch.nn.modules.conv.Conv2d::conv3 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.1/Relu_2_output_0 : Float(*, 256, 8, 8, strides=[16384, 64, 8, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.1/Relu_2\"](%/models.1/conv3/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.1/MaxPool_2_output_0 : Float(*, 256, 4, 4, strides=[4096, 16, 4, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.1/MaxPool_2\"](%/models.1/Relu_2_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.1/conv4/Conv_output_0 : Float(*, 512, 4, 4, strides=[8192, 16, 4, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.1/conv4/Conv\"](%/models.1/MaxPool_2_output_0, %onnx::Conv_183, %onnx::Conv_184), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1/torch.nn.modules.conv.Conv2d::conv4 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.1/Relu_3_output_0 : Float(*, 512, 4, 4, strides=[8192, 16, 4, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.1/Relu_3\"](%/models.1/conv4/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.1/GlobalAveragePool_output_0 : Float(*, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool[onnx_name=\"/models.1/GlobalAveragePool\"](%/models.1/Relu_3_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1382:0\n",
      "  %/models.1/flatten/Flatten_output_0 : Float(*, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/models.1/flatten/Flatten\"](%/models.1/GlobalAveragePool_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1/torch.nn.modules.flatten.Flatten::flatten # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/flatten.py:53:0\n",
      "  %/models.1/fc1/Gemm_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/models.1/fc1/Gemm\"](%/models.1/flatten/Flatten_output_0, %models.1.fc1.weight, %models.1.fc1.bias), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1/torch.nn.modules.linear.Linear::fc1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/models.1/Relu_4_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.1/Relu_4\"](%/models.1/fc1/Gemm_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.1/fc2/Gemm_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/models.1/fc2/Gemm\"](%/models.1/Relu_4_output_0, %models.1.fc2.weight, %models.1.fc2.bias), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.1/torch.nn.modules.linear.Linear::fc2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/models.2/conv1/Conv_output_0 : Float(*, 64, 32, 32, strides=[65536, 1024, 32, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.2/conv1/Conv\"](%x, %onnx::Conv_186, %onnx::Conv_187), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2/torch.nn.modules.conv.Conv2d::conv1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.2/Relu_output_0 : Float(*, 64, 32, 32, strides=[65536, 1024, 32, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.2/Relu\"](%/models.2/conv1/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.2/MaxPool_output_0 : Float(*, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.2/MaxPool\"](%/models.2/Relu_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.2/conv2/Conv_output_0 : Float(*, 128, 16, 16, strides=[32768, 256, 16, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.2/conv2/Conv\"](%/models.2/MaxPool_output_0, %onnx::Conv_189, %onnx::Conv_190), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2/torch.nn.modules.conv.Conv2d::conv2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.2/Relu_1_output_0 : Float(*, 128, 16, 16, strides=[32768, 256, 16, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.2/Relu_1\"](%/models.2/conv2/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.2/MaxPool_1_output_0 : Float(*, 128, 8, 8, strides=[8192, 64, 8, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.2/MaxPool_1\"](%/models.2/Relu_1_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.2/conv3/Conv_output_0 : Float(*, 256, 8, 8, strides=[16384, 64, 8, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.2/conv3/Conv\"](%/models.2/MaxPool_1_output_0, %onnx::Conv_192, %onnx::Conv_193), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2/torch.nn.modules.conv.Conv2d::conv3 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.2/Relu_2_output_0 : Float(*, 256, 8, 8, strides=[16384, 64, 8, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.2/Relu_2\"](%/models.2/conv3/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.2/MaxPool_2_output_0 : Float(*, 256, 4, 4, strides=[4096, 16, 4, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/models.2/MaxPool_2\"](%/models.2/Relu_2_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/models.2/conv4/Conv_output_0 : Float(*, 512, 4, 4, strides=[8192, 16, 4, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/models.2/conv4/Conv\"](%/models.2/MaxPool_2_output_0, %onnx::Conv_195, %onnx::Conv_196), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2/torch.nn.modules.conv.Conv2d::conv4 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/models.2/Relu_3_output_0 : Float(*, 512, 4, 4, strides=[8192, 16, 4, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.2/Relu_3\"](%/models.2/conv4/Conv_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.2/GlobalAveragePool_output_0 : Float(*, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool[onnx_name=\"/models.2/GlobalAveragePool\"](%/models.2/Relu_3_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1382:0\n",
      "  %/models.2/flatten/Flatten_output_0 : Float(*, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/models.2/flatten/Flatten\"](%/models.2/GlobalAveragePool_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2/torch.nn.modules.flatten.Flatten::flatten # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/flatten.py:53:0\n",
      "  %/models.2/fc1/Gemm_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/models.2/fc1/Gemm\"](%/models.2/flatten/Flatten_output_0, %models.2.fc1.weight, %models.2.fc1.bias), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2/torch.nn.modules.linear.Linear::fc1 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/models.2/Relu_4_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/models.2/Relu_4\"](%/models.2/fc1/Gemm_output_0), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/models.2/fc2/Gemm_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/models.2/fc2/Gemm\"](%/models.2/Relu_4_output_0, %models.2.fc2.weight, %models.2.fc2.bias), scope: __main__.EnsembleWrapper::/__main__.ImprovedEncoderStealer::models.2/torch.nn.modules.linear.Linear::fc2 # /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/Add_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/Add\"](%/models.0/fc2/Gemm_output_0, %/models.1/fc2/Gemm_output_0), scope: __main__.EnsembleWrapper:: # /var/folders/df/_mtd2_kx7s174908b5csl7yc0000gn/T/ipykernel_23142/4210780452.py:8:0\n",
      "  %/Add_1_output_0 : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/Add_1\"](%/Add_output_0, %/models.2/fc2/Gemm_output_0), scope: __main__.EnsembleWrapper:: # /var/folders/df/_mtd2_kx7s174908b5csl7yc0000gn/T/ipykernel_23142/4210780452.py:8:0\n",
      "  %/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}, onnx_name=\"/Constant\"](), scope: __main__.EnsembleWrapper:: # /var/folders/df/_mtd2_kx7s174908b5csl7yc0000gn/T/ipykernel_23142/4210780452.py:8:0\n",
      "  %output : Float(*, 1024, strides=[1024, 1], requires_grad=1, device=cpu) = onnx::Div[onnx_name=\"/Div\"](%/Add_1_output_0, %/Constant_output_0), scope: __main__.EnsembleWrapper:: # /var/folders/df/_mtd2_kx7s174908b5csl7yc0000gn/T/ipykernel_23142/4210780452.py:8:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Export with correct input name\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "torch.onnx.export(\n",
    "    final_model,\n",
    "    dummy_input,\n",
    "    \"stolen_model_improved.onnx\",\n",
    "    input_names=[\"x\"],  # Must be \"x\" to match server expectations\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        'x': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dbc91653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX validation passed! Model meets submission requirements\n"
     ]
    }
   ],
   "source": [
    "# Validate ONNX\n",
    "validate_onnx(\"stolen_model_improved.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "acb54b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successful!\n",
      "{'L2': 8.897150039672852}\n"
     ]
    }
   ],
   "source": [
    "# Submit model\n",
    "submit_model(\"stolen_model_improved.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "890c39fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da0365d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179bfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
